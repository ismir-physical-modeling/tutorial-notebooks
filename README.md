# Differentiable Physical Modeling Sound Synthesis: Theory, Musical Application, and Programming

- ISMIR 2025 Tutorial
- Date: 2025-09-20
- Presenters: [Jin Woo Lee], [Stefan Bilbao], and [Rodrigo Diaz]

### Abstract
Recent years have witnessed growing interest in bridging traditional sound synthesis methods with emerging machine learning technologies. This tutorial is motivated by the convergence of two previously distinct trajectories in audio research: physics-based sound synthesis and data-driven neural approaches. This session highlights how differentiable physical modeling opens new avenues for musical sound synthesis by combining the interpretability and realism of physical simulation with the learning capacity of modern neural networks.

The tutorial is structured into five segments: an overview of digital synthesis history and physical modeling, a detailed introduction to finite difference time domain (FDTD) methods across various instrument classes, a broad survey of neural architectures relevant to physical modeling, an in-depth look at differentiable modeling for parameter estimation using automatic differentiation, and a concluding session to synthesize key takeaways. Attendees will engage with theoretical material, practical demonstrations, and programming exercises, gaining hands-on experience in combining physics-based simulation with neural networks.

This tutorial is designed for researchers and engineers interested in advanced sound synthesis, particularly those working in musical acoustics, AI-based audio modeling, or digital instrument design. It will benefit individuals seeking to build physically plausible audio models or hybrid machine learning systems for realistic sound generation. All ISMIR members are warmly encouraged to attend—whether newcomers or seasoned researchers. The tutorial is designed to be approachable rather than overly technical, while still offering a deep understanding of how differentiable simulation can enhance synthesis fidelity, support neural network training, and advance hybrid sound modeling.

### Bios

[Jin Woo Lee] is currently a Postdoctoral Associate at Massachusetts Institute of Technology (MIT). He received his PhD degree from Seoul National University, with a thesis entitled “Physical Modeling for String Instrument Sound Synthesis based on Finite-difference Scheme and Automatic Differentiation”. His research interest is mainly focused on machine learning, physical modeling, and numerical simulation, involving audio, music, speech, and acoustics. His works have been presented at conferences such as NeurIPS, ICASSP, Interspeech, and WASPAA, as well as invited talks at Stanford CCRMA and the University of Iowa. Previously, he worked at Meta and Supertone as an intern, and at Gaudio Lab as an AI Scientist.

[Stefan Bilbao] (B.A. Physics, Harvard, 1992, MSc., PhD Electrical Engineering, Stanford, 1996 and 2001 respectively) is currently Professor of Acoustics and Audio Signal Processing in the Acoustics and Audio Group at the University of Edinburgh, and previously held positions at the Sonic Arts Research Centre, at the Queen’s University Belfast, and the Stanford Space Telecommunications and Radioscience Laboratory. He led the ERC-funded NESS and WRAM projects between 2012 and 2018. He is an Associate Editor of JASA Express Letters, and a Senior Area Editor of the IEEE Open Journal of Signal Processing, and was previously an associate editor of the IEEE/ACM Transactions on Audio Speech and Language Processing. He was awarded the Foreign medal of the French Acoustical Society in 2022. He works primarily on problems in acoustic simulation and audio signal processing for sound synthesis and room acoustics applications. He was born in Montreal, Quebec, Canada.

[Rodrigo Diaz] is a PhD candidate in Artificial Intelligence and Music at Queen Mary University of London, under the supervision of Prof. Mark Sandler and Dr. Charalampos Saitis. His research focuses on real-time audio synthesis using neural networks and physics-based modelling. His work has been presented at conferences across the audio and computer vision communities, including CVPR, ICASSP, IC3D, DAFx, and AES. Before his PhD, he worked as a researcher at the Fraunhofer HHI Institute in Berlin, exploring volumetric reconstruction from images using neural networks.

[Jin Woo Lee]: https://jnwoo.com/
[Stefan Bilbao]: https://www.acoustics.ed.ac.uk/people/dr-stefan-bilbao/
[Rodrigo Diaz]: https://rodrigodzf.com/


